{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projet réalisé par Fréjoux Gaëtan et Niord Mathieu.  \n",
    "Projet portant sur l'apprentissage par renforcement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Développement d'un jeu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Créer une fonction qui permet de simuler le plateau en positionnant notamment les différents éléments (case départ, fin, dragons)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD = [   \n",
    "        'S', ' ', ' ', ' ', \n",
    "        'D', ' ', 'D', ' ', \n",
    "        ' ', ' ', ' ', 'D', \n",
    "        ' ', 'D', ' ', 'J'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etant donné que le plateau ne change pas, nous avons décidé de le représenter directement avec une matrice sans utiliser une fonction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Créer une fonction qui permet de simuler l'interaction entre l'agent et son environnement*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP = 0              # GO UP\n",
    "RIGHT = 1           # GO RIGHT\n",
    "DOWN = 2            # GO DOWN\n",
    "LEFT = 3            # GO LEFT\n",
    "\n",
    "JAIL_REWARD = 1     # Reward for the jail cell\n",
    "DRAGON_REWARD = -1  # Reward for the dragon cell\n",
    "NORMAL_REWARD = 0   # Reward for the normal cell\n",
    "\n",
    "NB_GAME = 2000      # Number of games\n",
    "ALPHA = 0.81        # Learning rate\n",
    "GAMMA = 0.96        # Discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def application_action(action, position, space):\n",
    "    def type_to_reward(type):\n",
    "        if type == 'J':\n",
    "            return JAIL_REWARD\n",
    "        elif type == 'D':\n",
    "            return DRAGON_REWARD\n",
    "        else:\n",
    "            return NORMAL_REWARD\n",
    "\n",
    "    if (action == UP):\n",
    "        if (position - 4 >= 0):\n",
    "            position -= 4\n",
    "    elif (action == RIGHT):\n",
    "        if (position % 4 != 3):\n",
    "            position += 1\n",
    "    elif (action == DOWN):\n",
    "        if (position + 4 < 16):\n",
    "            position += 4\n",
    "    elif (action == LEFT):\n",
    "        if (position % 4 != 0):\n",
    "            position -= 1\n",
    "    else:\n",
    "        print('Error : action not recognized')\n",
    "    if (space[position] == 'D'):\n",
    "        return 0, -1, True\n",
    "    elif (space[position] == 'J'):\n",
    "        return 0, JAIL_REWARD, True\n",
    "    else:\n",
    "        return position, NORMAL_REWARD, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Développement du Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Créer une fonction nommé **choose_action** qui applique la stratégie epsilon-greedy pour choisir l'action à effectuer selon l'état de l'agent (ou sa position)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "def choose_action(state, epsilon, mat_q):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, 3)\n",
    "    else:\n",
    "        return np.argmax(mat_q[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Créer une fonction nommé **onestep** qui applique une itération de l'algorithme Q-learning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onestep(mat_q, state, epsilon):\n",
    "    action = choose_action(state, epsilon, mat_q)\n",
    "    new_state, reward, is_end = application_action(action, state, BOARD)\n",
    "    mat_q[state, action] = mat_q[state, action] + ALPHA * \\\n",
    "        (reward + GAMMA * np.max(mat_q[new_state]) - mat_q[state, action])\n",
    "    return new_state, reward, is_end, mat_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Tester votre algorithme avec des récompenses R = −1, 0, 1. Nous proposons d’étudier le paramétrage suivant : α = 0.81, γ = 0.96.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.603157   3.75328854 2.603157   3.603157  ]\n",
      " [3.75328854 3.603157   3.90967556 3.603157  ]\n",
      " [3.603157   3.45903072 2.603157   3.75328854]\n",
      " [3.45903072 3.45903072 3.32066949 3.603157  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [3.75328854 2.603157   4.07257871 2.603157  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [3.45903072 3.32066949 2.603157   2.60315699]\n",
      " [2.603157   4.07257871 3.75328854 3.90967556]\n",
      " [3.90967556 4.24226949 2.603157   3.90967556]\n",
      " [2.603157   2.603157   4.41903072 4.07257871]\n",
      " [0.         0.         0.         0.        ]\n",
      " [3.90967556 2.603157   3.75328854 3.75328854]\n",
      " [0.         0.         0.         0.        ]\n",
      " [4.24226949 4.603157   4.41903072 2.603157  ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "NB_GAME = 10000\n",
    "MAX_STEP = 100\n",
    "\n",
    "\n",
    "def run(q, epsilon, max_step):\n",
    "    \"\"\"\n",
    "    Run a game\n",
    "    \"\"\"\n",
    "    state = 0\n",
    "    is_end = False\n",
    "    nb_step = 0\n",
    "    reward = 0\n",
    "    while not is_end and nb_step < max_step:\n",
    "        state, reward, is_end, q = onestep(q, state, epsilon)\n",
    "        nb_step += 1\n",
    "    return nb_step, reward, q\n",
    "\n",
    "\n",
    "def train(nb_game, max_step):\n",
    "    \"\"\"\n",
    "    Train the agent\n",
    "    \"\"\"\n",
    "    nb_steps = []\n",
    "    q = np.zeros((16, 4))\n",
    "    minimum = 0\n",
    "    success_number_of_steps = []\n",
    "    for i in range(nb_game):\n",
    "        nb_step, reward, q = run(q, nb_game / (i + nb_game), max_step)\n",
    "    return q\n",
    "\n",
    "\n",
    "q = train(NB_GAME, MAX_STEP)\n",
    "print(q)\n",
    "\n",
    "\n",
    "# show the board with the best action for each state add the legend for each action\n",
    "#plt.imshow(np.argmax(q, axis=1).reshape(4, 4))\n",
    "# plt.colorbar()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Etudier la table Q*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe suite à l'entrainement que la table Q est remplie de valeurs.\n",
    "On remarque tout d'abord que les cases comprenants des dragons ou la case de fin sont remplis de **0**. Cela est normal car il **n'est pas possible** de se déplacer sur ces cases.\n",
    "Enfin, on remarque que pour chaque état, la meilleur action possible est celle ayant la plus grande valeur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Jouer une partie avec la politique optimale liée à la table et commenter le parcours*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of steps to reach the jail :  6\n"
     ]
    }
   ],
   "source": [
    "nb_step, _, _ = run(q, 0, 100)\n",
    "\n",
    "print(\"Number of steps to reach the jail : \", nb_step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe qu'avec un entrainement assez conséquent, on obtiens une politique optimale qui permet de gagner la partie en 6 coups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[720.63139903 750.65770732 719.63139903 720.63139903]\n",
      " [750.65770732 720.63139903 781.93511179 720.63139903]\n",
      " [720.63139903 691.80614307 719.63139903 750.65770732]\n",
      " [691.80614307 691.80614307 690.84614307 720.63139903]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [750.65770732 719.63139903 814.51574145 719.63139903]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [691.80614304 690.84614304 719.63139903 719.63139888]\n",
      " [719.63139903 814.51574145 750.65770732 781.93511179]\n",
      " [781.93511179 848.45389734 719.63139903 781.93511179]\n",
      " [719.63139903 719.63139903 883.80614307 814.51574145]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [781.93511179 719.63139903 750.65770732 750.65770732]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [848.45389734 920.63139903 883.80614307 719.63139903]\n",
      " [  0.           0.           0.           0.        ]]\n"
     ]
    }
   ],
   "source": [
    "DRAGON_REWARD = -10  # Reward for the dragon cell\n",
    "NORMAL_REWARD = 0   # Reward for the normal cell\n",
    "JAIL_REWARD = 200     # Reward for the jail cell\n",
    "\n",
    "ALPHA = 0.81        # Learning rate\n",
    "GAMMA = 0.96        # Discount factor\n",
    "\n",
    "#TODO\n",
    "q = train(10000, MAX_STEP)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Faites évoluer votre algorithme en modifiant les récompenses pour gagner en efficacité.*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Etudier la table Q.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Jouer une partie avec la politique optimale liée à la table et commenter le parcours.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO COMMENTAIRE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comparer avec la première politique*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO COMPARAISON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Modifier la fonction **choose action(state,epsilon,modele)** qui va choisir dans certains cas l'action à appliquer avec :  \n",
    "**Sortie Q = model.predict(np.array([vec etat]))***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(16, activation='relu', input_shape=(16,)),\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dense(4, activation='linear')\n",
    "])\n",
    "\n",
    "#new version with predict model.predict(np.array([vec etat]))\n",
    "def choose_action(state, epsilon, model):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, 3)\n",
    "    else:\n",
    "        return np.argmax(model.predict(np.array([state])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*la procédure permettant de mettre à jour les poids du réseau grâce à la différentiation automatique **tf.GradientTape()***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(state, action, reward, new_state, model):\n",
    "    target = reward + GAMMA * np.max(model.predict(np.array([new_state])))\n",
    "    target_vec = model.predict(np.array([state]))\n",
    "    target_vec[0][action] = target\n",
    "    model.fit(np.array([state]), target_vec, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tester votre algorithme avec des récompenses R = −20, −1, 100*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Jouer une partie avec la politique optimale liée à la table et commenter le parcours*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO COMMENTAIRE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Faites évoluer votre algorithme en introduisant un second réseau*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f64dd527c38d67caa9a599e0f653ae824a4035e5d2a0a8359c2a297e8befdc6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
